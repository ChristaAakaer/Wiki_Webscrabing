---
title: "Wiki_Webscraping_v01"
author: "Magnus Lindholm Nielsen"
date: "15/12/2020"
output: html_document
---
This simple scripts collects several random pages from Wikiquotes and creates a dataframe
of their body text and titles. The data is quite messy, since the web pages HTML structure
is not consistent.

```{r}
gentag <- 10 # How many pages you wish to download. 1000 takes approximately 10-15 minutes.
sum <- 0
tekst_v <- vector()
overskrift_v <- vector()
repeat{
  wiki_random <- read_html("https://en.wikiquote.org/wiki/Special:Random")
  tekst_random <- html_text(html_node(wiki_random, 'li'))
  overskrift_random <- html_text(html_node(wiki_random, 'i'))
  tekst_v <- append(tekst_v, tekst_random)
  overskrift_v <- append(overskrift_v, overskrift_random)
  sum = sum+1
  if(sum == gentag){
    break
  }
}
nummer <- sprintf('nummer % d', 1:gentag)
wiki.dt <- data.frame(nummer, overskrift_v, tekst_v)
print(wiki.dt)


```

